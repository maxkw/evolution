\documentclass[doc, natbib]{apa6}

\usepackage{natbib}
\usepackage{pslatex}
\usepackage{anyfontsize, todonotes}
\usepackage{graphicx, amsmath, subcaption, amssymb, comment, bm, amsthm, bbm}
\usepackage{float}				
\usepackage{todonotes} 

\usepackage{sgame}
\usepackage{tikz}
\usetikzlibrary{calc, trees}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}

\newlength{\subfigheight}

\newcommand{\mtodo}[2][]
{\hfil \todo[caption={\textbf{MKW}}, size=\small, color = yellow, inline, #1]{#2}}

\title{The Evolution of Social and Moral Preferences}
\shorttitle{Social Agency}

\authornote{Max Kleiman-Weiner (\url{maxkw@mit.edu})}
% \affiliation{Department of Brain and Cognitive Sciences\\ Massachusetts Institute of Technology \\Cambridge, MA 02139}
\affiliation{}
 
% \authornote{\\Address correspondence to: \\
% Max Kleiman-Weiner \\
% maxkw@mit.edu \\
% Department of Brain and Cognitive Sciences \\
% Massachusetts Institute of Technology \\
% 77 Massachusetts Ave, 46-4053 \\
% Cambridge, MA 02139}

\abstract{
There have been three key principles for understanding the mind: computation, evolution and specialization \citep{pinker1997mind}. Over the past decades, there has been significant progress in understanding the evolutionary origins of proto-moralities from the study of altruistic behavior and cooperation \citep{axelrod1985evolution, nowak2005evolution, rand2013human}. A strength of this work is its reliance on simple behavioral models which can explain cooperation across many scales of life: from symbiotic single celled organisms, animals that hunt in groups, and even aspects of human prosociality. While the simplicity of these behavioral models speaks to their universality, their inability to account for the specialized computational abilities unique to humans may fail to account the richness and scale of human cooperation. In this model we consider the evolution of agents who express social and moral preferences through utility principles i.e., abstract measures for cost and benefit that can apply to any situation and infer the intentions of others agents through Bayesian Theory-of-Mind. We show that an evolutionary analysis of these computational specializations: theory-of-mind and abstract principles of utility can account for the robustness of human social behavior and judgment, and unifies existing behavioral models of reciprocity into proto-morality. Specifically, this model accounts for the importance of first impressions, how social experience generalizes across contexts, and why being good feels good. 
} 
\begin{document}
\maketitle

\section{Computational Model}

A rational agent aims to maximize their utility subject to their beliefs. Agents with different utility functions have different goals which through planning will lead to different behavior. Let $R_i$ be the reward given to agent $i$. Then one simple characterization of an agent is one that only cares about its own reward:
%
\begin{equation}
U_i(s) = R_i
\end{equation}
%
where $U_i(s)$ is the subjective utility experienced by agent $i$ in state $s$. We call this agent type \emph{selfish} since it acts to efficiently maximize only its own rewards without concern for any of the others. We can define a \emph{kind} agent that always acts to maximize joint welfare: 
%
\begin{equation}
U_i(s) = R_i + \sum_{j \neq i} U_j(s)
\end{equation}
%
where the sum over $U_j(s)$ is the recursive valuation of the utility of others as ones own. This leads the \emph{kind} agent to maximize joint welfare. Finally, we define a \emph{reciprocal} agent that only values the utility of other reciprocal agents:
%
\begin{equation}
U_i(s) = R_i + \sum_{j \neq i} U_j(s)\mathbbm{1}_{j \in R}
\end{equation}
%
where $\mathbbm{1}_{j \in R}$ is an indicator function that is $1$ if agent $j$ is in the group of reciprocal agents $R$ and $0$ otherwise. Thus an agent with this utility function instantiates a form of indirect reciprocity. Reciprocal agents act kind towards other reciprocal agents and selfish towards selfish agents. By specifying these agents using principles of utility, we formalize an abstract version of indirect reciprocity. However, these agent do have behavioral analogous if we limit our analysis to a repeated prisoners dilemma. An agent which always selects ``defect'' (ALLD) would be similar to the \emph{selfish} agent, an agent which always selects ``cooperate'' (ALLC) would be similar to the \emph{kind} agent and an adaptive strategy which copies the previous behavior of its partner (TFT) but starts off cooperating would share some similarities with the \emph{reciprocal} agent. However these behavioral strategies do not generalize beyond the repeated prisoners dilemma and cannot account of the ability of social strategy to generalize across actions spaces and context. Furthermore, the more sophisticated behavioral strategies such as TFT do not robustly handle noise and uncertainty. 

Although easy to define, reciprocity is challenging because the type of the other agent cannot be directly observed i.e., $\mathbbm{1}_{j \in R}$ cannot be known. Instead whether or not an agent is reciprocal has to be inferred. Since agents act rationally to achieve their goals, their actions give information about their underlying goals. 

% Alger and Weibull cite. They describe the evolution of social preferences which go beyond behaviors strategies. But only analyze utility functions of the type Me + alpha*you for two player interactions. There is no notion of reciprocity since each players type is private and players to not learn the types of others. Other references to checkout are: Bisin and Verdier (2001) and (2004) which talk about parents choosing to instill their values in their children by paying a small teaching cost. Might connect nicely with the type of preferences that Fiery likes where value functions that are good at spreading, spread. The whole paper is a really complete set of the greatest hits of social preferences in economics. 

% \noindent Organizing statement: What minimal set of abstract social preferences and social planning mechanisms can are necessary to unify the complexity of social behavior.

\begin{itemize}
\item Abstract Reciprocity: How can we go beyond Tit-For-Tat? How do we recognize a ``defection'' and how do we recognize a ``cooperation''? How to make these strategies robust to noise in both action and intention (trembling hand)? Can an abstract principle of reciprocity explain both direct and indirect reciprocity without proposing additional mechanisms? Do TFT like strategies emerge from social preferences and social inference?

If social preferences unify forms of direct and indirect reciprocity does this unify the Krasnow / Rand theories under the same model? Both reputation and partner control of future interactions are important. 
\item Habits of virtue "spill-over" can be explained through hierarchcial bayes, you may start with a prior over the prior agents but you can also learn hierarchically about the distribution in the society which will explain the spill-over effects. This can explain how humans learn quickly from their environment and generalize what they've learned to new games and contexts. 

\item What is the utility function for punishment? Planning vs. social preferences? Partner control vs. partner choice? Punishment and retribution vs. teaching? 

\end{itemize}

% Josh notes
How do we do so much from so little. Two senses, how do we learn so much from others from so little data? How do we behavior flexibly in so many situations using just simple preferences that condition on rich mental states. 

They get back to cooperation eventually, but it matters a lot more in the beginning. first impressions matter when there is partner choice, but if you are forced to be together you can figure it out. these are important facts of social life. 

% FIERY NOTES

Greenbeards? Are values selected in and of themselves? I.e., if there is a mechanism for learning values (how does value learning ultimately hash out fitness). What are the value learning mechanisms. Who should you learn your values from? How does the rate of values learning translate into a selection paradigm....

% could we build a set of games, where the actions and options are observable. round to round, you can observe the options:
% There will be a relation between how forgiving the agent is in terms of its ability to rapidly detect cheaters and when reciprocal will be favored. There is an inverse relationship between the ability to detect cheaters and the ability to detect self-type. Its a false-positive, true-negative tradeoff? Is this trade-off direct? Might evolution favor a certain balance?

% 1. if you want to write down the reactive strategy.... its incredible painfully complicated
% 2. if you assume that your response to me is conditioned on utility its more sensible
% 3. involves incorporating information about the alternatives I have. defect vs. shoot myself (obviously defect is now OK). 100 actions in the environment 

% Howard racklin (pidgeons) - sequential PD against TFT is operant conditioning.... what simultaneous game vs. sequential game. when you make it simultaneous, they always defect. 

% When doing inference, assume the other agent knows my type instead of them sampling it.  

% forgive - graph also include other agent is defecting type (also include payoff to show that mutual cooperaiton is a reall good thing)

% cushman owen macendoe (2008)

% look at peoples propensity to cooperate (weekly condition on cooperative partner) - 

% invstigating how higher order theories of mind make cooperation more robust and reliable... built more theory of of mind increase the parameter space. 

% mismatch between the science of social behavior and the formal models of the evolution of social behavior - the empirical science shows we make behavioral or reactive strategies. is that only because reactive strategies simpler and the same thing happens anyway. social preferences are deeply integrated with mental state inference. the combination of mental state inference describes more. codependence between mental states and social prefernece. 

 % goal 2 show that indirect reciporcity comes and because its an abstract theory of cooperation we can handle both the krasnow and rand theories with the same model (it matters for direct and indirect reciprocity and it accounts for signaling as well as selfishly motivated)

 % double check the nowak indirect reciprocity model where he shows there is a problem (is it an order effect problem?). show how being bayesian can alleviate that. 

 
 % What about cooperate WITH looking. What inference do you make, when the agent looks at the payoffs or the payoffs are revealed and its very very costly for the agent to cooperate anyway. Isn't that agent even better because he signals his value function while the cooperate without looking agent signals that he doesn't have sufficient self-control to be trustworthy. THis is important because sometimes the costs and benefits are obvious. 


 % other fun ideas: evolution of merit, evolution of need, partiality

 % TODO: Have repeated game types, simultaneous move games and sequential games. Would be good to have a general framework for all of the above.

% What is the right decision rule for the reciprocal agents? Currently sampling the reciprocity parameter of the other agent from its distribution and then using that as the decision... 

% Have a utiltiy function that includes other people, but what is behavior ultimately conditioned on. 

% - Evolution of utility functions that can explain all the forms of punishment
% - Learning a deontological norm (vs consequnece) using the utility function from the moral learning paper. (could also be done in children / maybe with sydney)
% - Moral equilibrium learning by reducing inconsitencey (the balance theory)

% do you need theory of mind when there is variability when there is standing. this goes beyond the normal model of standing in that there can be false beliefs about what others think about standing. 

% dreber forgiving of bad action when intent is good. could formalize this as a false belief about the world. we can also forgive "bad" conseuqnces when there is environmental noise if we know the agent a false belief about how the owlrd works. 

% mulitply embeded mental states

% where does punishment come from: where does proportional punishment come from? make whole vs. make equalize. proportionality of imposed cost. 

% reciprical agents are all do the sharing, you could look at what would happen if some demanded more and some demanded less. 50/50 comes from solving coordination problems here if its imbalanced there would be coordination failure in assymetric battle of the sexes. 

% simplest... alternative weighting of utility. have agents that weight it differently. reciprocial type as least as generous as I am. non-zero reciprocity is a race to the bottom. otherwise its a race to the top but the top of the race might be 50/50. or the decision rule is some window around ones own weighting. 

% doesn't actually have to learn about the types of others. just need to learn whether or not they are above or below the threshold (actually might need to in order to understand the behavior of other agents). 

\section{Bankers Paradox}
Is this really a paradox? What do bankers do... increase the interest rate to account for the increased risk, having utilities not strategies compensates for this since it allows for quantitative trade-offs. The story of how externalities might promote mutual cooperation is a nice idea of bacteria who lack mechanisms for social inference. For agents capable of understanding the values of their actions to others and mechanisms for planning will lead to economic exchange and gains from trade (which itself might be a mechanism for stable cooperation), but not an inflexible behavioral response like how bacteria might work 

\section{Questions for David}
\begin{itemize}
\item Writing down PD as a simultaneous dictator game might be identical for a certain class of PD (i.e., those that correspond to a public goods game) but as far as I can tell is *not* a general way to specify games with a particular failing for coordination games. In the simultaneous dictator version of the game an agent can consider the utility of just their part of the of game without any consideration of what the other person will do. This isn't the case in general (e.g., battle of the sexes, trust games, etc). Cognitively, thinking about the game as a simultaneous dictator game feels different to me than thinking about it as a normal form game even though the mechanisms are the same (cite Tversky sequential PD). I think this might be because the simultaneous dictator game unwinds some of the strategic reasoning and equilibrium calculation which makes it easier to see the strategic significance of the two actions. Have you thought about this? 

The current code implements the simultaneous dictator game since it makes the utility evaluation very simple for each agent but this won't generalize to other games like coordination games where the payoff is truly a function of both players. I think we can use QRE (quantal response equilibrium) as a general way of mapping a utility transformed payoff matrix into actions but wanted to get results on the model we discussed specifically. 

\item What are your "first try" parameters for an evolutionary simulation and can you point me to some practical background on this? I'm guessing the first thing to try is kill an agent and regrow with $ \propto \exp(\beta * \text{Fitness})$? Is this the "Moran Process"? Do you usually start with a uniform or random distribution of agents and see what the final proportions are? Or do you look at invasions first? 
\end{itemize}


\section{Computation Analysis}
We use the standard probabilistic decision-making tools to map preferences into actions. When actions lead to different outcomes probabilistically, we use the expected utility of an action:
\begin{align*}
  EU(a,s) = \sum_{s'} U^k(s',a)P(s'|a, s)
\end{align*}
Decision-making is then defined probabilistically under the Luce-choice decision rule which reflects utility maximization when there is uncertainty about the exact utility value \cite{luce1959individual}:
%
\begin{align}
  P(a | s) = \frac{\exp(\beta EU(a,s))}{\sum_{a' \in A} \exp(\beta EU(a',s))}
\label{softmax}
\end{align}
%
When $\beta \rightarrow 0$ the decision maker chooses randomly and when $\beta \rightarrow \infty$ the decision maker will always choose the highest utility action.

We define two agents, an agent with no social preferences whose only goal is to maximize his own utility and an agent with social preferences who values the utility of those who value the utility of others...

\subsection{Online Inference}

\begin{align}
  P(T=t | O_{1:N}) &= \frac{P(T=t)\prod_{i \in 1:N} P(O_i | T=t)}{\sum_T P(T=t)\prod_{i \in 1:N} P(O_i | T=t)} \\
  \mathcal{L}_N(T=t) &= \prod_{i \in 1:N} P(O_i | T=t) \\
  \mathcal{L}_{N+1}(T=t) &= \mathcal{L}_N(T=t) \cdot P(O_i | T=t) \\
  P(T | O_{1:N}, O_{N+1}) &= \frac{P(T=t)\mathcal{L}_{N+1}(T=t)}{\sum_T P(T=t)\mathcal{L}_{N+1}(T=t)}
\end{align}

\begin{figure}[ht]
  \centering
  \includegraphics[width=.5\linewidth]{figures/forgiveness.pdf}
  \caption{\label{fig:forgiveness} }
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=.5\linewidth]{figures/protection.pdf}
  \caption{\label{fig:forgiveness} }
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=.5\linewidth]{figures/fitness_rounds.pdf}
  \caption{\label{fig:forgiveness} }
\end{figure}




\bibliography{/Users/max/Dropbox/projects/fairness/writing/social}{}

\end{document}
